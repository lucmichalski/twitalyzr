{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter #cassandra classification with Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as funs\n",
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer\n",
    "from pyspark.mllib.classification import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "regularization = 0.244, hash = 400, words = 1900 ---- PR-99, ROC -- 97\n",
    "'''\n",
    "filename_datastax = '/FileStore/tables/v1a51v731472210317176/data_stax_dataset.json'\n",
    "filename_bigdata = '/FileStore/tables/v1a51v731472210317176/big_data_dataset.json'\n",
    "filename_porn = '/FileStore/tables/v1a51v731472210317176/porn_tweets.json'\n",
    "filename_cass = '/FileStore/tables/v1a51v731472210317176/cassandra_dataset.json'\n",
    "filename_spark = '/FileStore/tables/v1a51v731472210317176/spark_tweets_dataset.json'\n",
    "filename_softeng = '/FileStore/tables/v1a51v731472210317176/sweng_dataset.json'\n",
    "filename_database = '/FileStore/tables/v1a51v731472210317176/database_dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performe text cleanup\n",
    "'''\n",
    "Example: @user RT: This IS a tweet. Hello-there! #hello #hi.\n",
    "Transforms into: this is tweet hello there hello hi\n",
    "'''\n",
    "def clean_dataframe(input_df):\n",
    "  onlywords_df = input_df.select(funs.lower(\n",
    "      funs.regexp_replace(\n",
    "        input_df.text,'(\\^|\\?|\\!|\\$|\\(|\\)|\\\\b\\\\w{1}\\\\b\\\\s?|\\\"|\\'|RT|:|\\.|@[^\\s]+|http[^\\s]+|#|[0-9]+)','')).alias('text'),'label','id','hashtags')\n",
    "  return onlywords_df.select(funs.trim(funs.regexp_replace(onlywords_df.text,'(\\+|_|-|\\,|\\s+)',' ')).alias('text'),'label','id','hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization of dataset\n",
    "'''\n",
    "\"this       is tweet\" --> [this, is, tweet]\n",
    "'''\n",
    "def tokenization(input_df):\n",
    "  tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "  return tokenizer.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make TF and IDF model from tokenized text\n",
    "'''\n",
    "'''\n",
    "def make_tf_idf(input_df, number_features):\n",
    "  hashing_tf_model = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=number_features)\n",
    "  hashed_tf_df = hashing_tf_model.transform(input_df)\n",
    "  idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "  idf_model = idf.fit(hashed_tf_df)\n",
    "  return hashing_tf_model,idf_model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Only make TF\n",
    "'''\n",
    "def feature_extr_onlytf(input_df,number_features):\n",
    "  hashing_tf_model = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=number_features)\n",
    "  return hashing_tf_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature extraction for given input DataFrame and TFIDF models\n",
    "'''\n",
    "def feature_extraction(input_df,tf_model,idf_model):\n",
    "  tf_features = tf_model.transform(input_df)\n",
    "  tfidf = idf_model.transform(tf_features)\n",
    "  return tfidf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make LabelPoints from DataFrame for training\n",
    "'''\n",
    "LabelPoint has two fields. One is label, and second is SparseVector named features\n",
    "'''\n",
    "def make_label_points(input_df):\n",
    "  return input_df.rdd.map(lambda row:LabeledPoint(row['label'],row['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reg_params(train_labels,cv_test,threshold_start,threshold_end,threshold_step,reg_start,reg_end,reg_step):\n",
    "  output = []\n",
    "  regularization = reg_start\n",
    "  while True:\n",
    "    threshold = threshold_start\n",
    "    while True:\n",
    "      model_lr = LogisticRegressionWithLBFGS()\n",
    "      model_lr = model_lr.train(train_labels,regParam=regularization)\n",
    "      model_lr.setThreshold(threshold)\n",
    "      predictionAndLabels = cv_test.rdd.map(lambda lp: (float(model_lr.predict(lp.features)), float(lp.label)))\n",
    "      metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "      output.append([regularization,threshold,metrics.areaUnderPR,metrics.areaUnderROC])\n",
    "      threshold+=threshold_step\n",
    "      if(threshold>threshold_end):\n",
    "        break\n",
    "    \n",
    "    regularization+=reg_step\n",
    "    if(regularization>reg_end):\n",
    "      break\n",
    "  \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_feature_num(train_set,cv_set,start_num,end_num,step):\n",
    "  output = []\n",
    "  for i in range(start_num,end_num,step):\n",
    "    tokenized_train = tokenization(train_set)\n",
    "    tokenized_cv = tokenization(cv_set)\n",
    "    #tf_model,idf_model = make_tf_idf(tokenized_train,i)\n",
    "    #train_tfidf = feature_extraction(train_tokenized,tf_model,idf_model)\n",
    "    #cv_tfidf = feature_extraction(tokenized_cv,tf_model,idf_model)\n",
    "    train_tfidf = feature_extr_onlytf(tokenized_train,i)\n",
    "    cv_tfidf = feature_extr_onlytf(tokenized_cv,i)\n",
    "    train_labels = make_label_points(train_tfidf)\n",
    "    lr = LogisticRegressionWithLBFGS()\n",
    "    model_lr = lr.train(train_labels)\n",
    "    model_lr.setThreshold(0.75)\n",
    "    predictionAndLabels = cv_tfidf.rdd.map(lambda lp: (float(model_lr.predict(lp.features)), float(lp.label), lp.id, lp.text))\n",
    "    predictionAndLabels_rdd = predictionAndLabels.map(lambda lp: (lp[0], lp[1]))\n",
    "    metrics = BinaryClassificationMetrics(predictionAndLabels_rdd)\n",
    "    output.append([i,metrics.areaUnderPR,metrics.areaUnderROC])\n",
    "  \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_posneg(input_df):\n",
    "  return {\"all\":input_df.count(),\"positive\":input_df.filter(input_df.label==1).count(),\"negative\":input_df.filter(input_df.label==0).count()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(sv1,sv2):\n",
    "  size = sv1.size + sv2.size\n",
    "  max_ind = sv1.size\n",
    "  indices1 = sv1.indices\n",
    "  indices2 = [max_ind+x for x in sv2.indices]\n",
    "  joined_indices = np.concatenate([indices1,indices2])\n",
    "  values1 = sv1.values\n",
    "  values2 = sv2.values\n",
    "  joined_values = np.concatenate([values1,values2])\n",
    "  return SparseVector(size,joined_indices,joined_values)\n",
    "\n",
    "s1 = SparseVector(2,[0],[1])\n",
    "s2 = SparseVector(3,[0,2],[1,1])\n",
    "print combine(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_word(hashtags):\n",
    "  hashtags = [x.lower() for x in hashtags]\n",
    "  return ' '.join(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_with_hash(input_df,number_hash,number_words):\n",
    "  lower_hashtags = (input_df.rdd.map(lambda row:[lower_word(row['hashtags']),row['text'],row['label'],row['id']])).toDF(['hashtags','text','label','id'])\n",
    "  \n",
    "  #tokenization\n",
    "  tok = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "  tok_hash = RegexTokenizer(inputCol='hashtags',outputCol='tags')\n",
    "  \n",
    "  tokenized_test_words = tok.transform(lower_hashtags)\n",
    "  tokenized_all = tok_hash.transform(tokenized_test_words)\n",
    "  \n",
    "  #hashingTF\n",
    "  tags_hash = HashingTF(inputCol=\"tags\", outputCol=\"features_tags\", numFeatures=number_hash)\n",
    "  words_hash = HashingTF(inputCol=\"words\", outputCol=\"features_words\", numFeatures=number_words)\n",
    "  tf_feat = tags_hash.transform(tokenized_all)\n",
    "  tf_all = words_hash.transform(tf_feat)\n",
    "  \n",
    "  #combine SparseVectors\n",
    "  tf_joined = (tf_all.rdd.map(lambda row:[combine(row['features_tags'],row['features_words']),row['text'],row['hashtags'],row['label'],row['id']])).toDF(['features','text','hashtags','label','id'])\n",
    "  \n",
    "  return tf_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_datastax_df = sqlContext.read.json(filename_datastax)\n",
    "raw_porn_df = sqlContext.read.json(filename_porn)\n",
    "raw_cassandra_df = sqlContext.read.json(filename_cass)\n",
    "raw_spark_df = sqlContext.read.json(filename_spark)\n",
    "raw_bigdata_df = sqlContext.read.json(filename_bigdata)\n",
    "raw_softeng_df = sqlContext.read.json(filename_softeng)\n",
    "raw_database_df = sqlContext.read.json(filename_database)\n",
    "\n",
    "print 'Datastax tweets:\\t',count_posneg(raw_datastax_df)\n",
    "print 'Porn tweets:\\t',count_posneg(raw_porn_df)\n",
    "print 'Cassandra tweets:\\t',count_posneg(raw_cassandra_df)\n",
    "print 'Spark tweets:\\t',count_posneg(raw_spark_df)\n",
    "print 'Bigdata tweets:\\t',count_posneg(raw_bigdata_df)\n",
    "print 'Software eng:\\t',count_posneg(raw_softeng_df)\n",
    "print 'Database:\\t',count_posneg(raw_database_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_datastax_df = (raw_datastax_df.filter(raw_datastax_df.label==1)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_spark_pos_df = (raw_spark_df.filter(raw_spark_df.label==1)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_spark_neg_df = (raw_spark_df.filter(raw_spark_df.label==0)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_bigdata_df = raw_bigdata_df.randomSplit([0.3,0.7],23)[0]\n",
    "sampled_softeng_df = raw_softeng_df.randomSplit([0.3,0.7],23)[0]\n",
    "sample_database_df = raw_database_df.randomSplit([0.1,0.7],23)[0]\n",
    "\n",
    "print 'Sampled Datastax',sampled_datastax_df.count()\n",
    "print 'Sampled Spark Positive',sampled_spark_pos_df.count()\n",
    "print 'Sampled Spark Negative',sampled_spark_neg_df.count()\n",
    "print 'Sampled Bigdata',sampled_bigdata_df.count()\n",
    "print 'Sampled Software Eng',sampled_softeng_df.count()\n",
    "print 'Sampled database',sample_database_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sampled_df = sampled_bigdata_df.unionAll(sampled_datastax_df).unionAll(sampled_softeng_df).unionAll(sampled_spark_neg_df).unionAll(sampled_spark_pos_df).unionAll(raw_porn_df).unionAll(sample_database_df)\n",
    "print 'Other sources sampled:',count_posneg(raw_sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_df,raw_cass_df = raw_cassandra_df.randomSplit([0.4,0.6],24)\n",
    "raw_dataset_df = raw_cass_df.unionAll(raw_sampled_df)\n",
    "\n",
    "print 'Whole dataset:',count_posneg(raw_dataset_df)\n",
    "print \"Test dataset:\",count_posneg(test_data_df)\n",
    "train_df = raw_dataset_df\n",
    "test_df = test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df,test_df = raw_dataset_df.randomSplit([0.75,0.25],45)\n",
    "#print 'Train set:',count_posneg(train_df)\n",
    "#print 'Test set:',count_posneg(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "Convert text tweets into label points for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_train_df = clean_dataframe(train_df)\n",
    "cleaned_test_df = clean_dataframe(test_df)\n",
    "train_tf_df = feature_extraction_with_hash(cleaned_train_df,400,1900)\n",
    "test_tf_df = feature_extraction_with_hash(cleaned_test_df,400,1900)\n",
    "train_tf_df.show()\n",
    "#tokens_train_df = tokenization(cleaned_train_df)\n",
    "#tokens_test_df = tokenization(cleaned_test_df)\n",
    "#train_tf_df = feature_extr_onlytf(tokens_train_df,1000).cache()\n",
    "#test_tf_df = feature_extr_onlytf(tokens_test_df,1000).cache()\n",
    "train_labels = make_label_points(train_tf_df).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Logistic regression with LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegressionWithLBFGS()\n",
    "model = lr.train(train_labels,regParam=0.244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out all predicted probabilities from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.setThreshold(0.75)\n",
    "predicted = test_tf_df.rdd.map(lambda l:(float(model.predict(l['features'])),float(l['label']),l['text'],l['id']))\n",
    "for pred in predicted.collect():\n",
    "  print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabels_rdd = predicted.map(lambda lp: (lp[0], lp[1]))\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels_rdd)\n",
    "\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_feature_num_hash(train_df,test_df,hash_start,hash_end,words_start,words_end,step):\n",
    "  threshold = 0.75\n",
    "  output = []\n",
    "  for i in range(hash_start,hash_end,step):\n",
    "    for j in range(words_start,words_end,step):\n",
    "      train = feature_extraction_with_hash(train_df,i,j)\n",
    "      test = feature_extraction_with_hash(test_df,i,j)\n",
    "      \n",
    "      train_labels = make_label_points(train)\n",
    "      lr = LogisticRegressionWithLBFGS()\n",
    "      model = lr.train(train_labels,regParam=0.244)\n",
    "      \n",
    "      model.setThreshold(threshold)\n",
    "      predictionAndLabels = test.rdd.map(lambda lp: (float(model.predict(lp.features)), float(lp.label), lp.id, lp.text))\n",
    "      predictionAndLabels_rdd = predictionAndLabels.map(lambda lp: (lp[0], lp[1]))\n",
    "      metrics = BinaryClassificationMetrics(predictionAndLabels_rdd)\n",
    "      output.append([i,j,metrics.areaUnderPR,metrics.areaUnderROC])\n",
    "  \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_reg_param_with_hash(labels_train,test_df,reg_start,reg_end,reg_step):\n",
    "  output = []\n",
    "  i = reg_start\n",
    "  while i<=reg_end:\n",
    "      lr = LogisticRegressionWithLBFGS()\n",
    "      model = lr.train(labels_train,regParam=i)\n",
    "      \n",
    "      model.setThreshold(0.75)\n",
    "      predictionAndLabels = test_df.rdd.map(lambda lp: (float(model.predict(lp.features)), float(lp.label), lp.id, lp.text))\n",
    "      predictionAndLabels_rdd = predictionAndLabels.map(lambda lp: (lp[0], lp[1]))\n",
    "      metrics = BinaryClassificationMetrics(predictionAndLabels_rdd)\n",
    "      output.append([i,metrics.areaUnderPR,metrics.areaUnderROC])\n",
    "      i+=reg_step\n",
    "    \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output = find_reg_param_with_hash(train_labels,test_tf_df,0.19,0.25,0.002)\n",
    "#for out in output:\n",
    "#  print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output = find_feature_num_hash(cleaned_train_df,cleaned_test_df,100,800,500,2000,100)\n",
    "#for out in output:\n",
    "#  print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output = find_feature_num(train_df,test_df,400,1500,100)\n",
    "#for out in output:\n",
    "#  print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.05 0.75\n",
    "#for out in reg_params(train_labels,test_tf_df,0.7,0.81,0.01,0.03,0.1,0.004):\n",
    "#  print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save(sc,'/FileStore/tables/oaohfspx1472031253448/trained_model.lrm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "build_model",
  "notebookId": 1748870514251955
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
