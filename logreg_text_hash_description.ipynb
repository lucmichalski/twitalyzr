{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter #cassandra filter\n",
    "## Using tweet text, hashtags and user description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as funs\n",
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter path of files (datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_update = '/FileStore/tables/rtqt0l191472460960965/cass_data28.json'\n",
    "filename_datastax = '/FileStore/tables/v1a51v731472210317176/data_stax_dataset.json'\n",
    "filename_bigdata = '/FileStore/tables/v1a51v731472210317176/big_data_dataset.json'\n",
    "filename_porn = '/FileStore/tables/v1a51v731472210317176/porn_tweets.json'\n",
    "filename_cass = '/FileStore/tables/v1a51v731472210317176/cassandra_dataset.json'\n",
    "filename_spark = '/FileStore/tables/v1a51v731472210317176/spark_tweets_dataset.json'\n",
    "filename_softeng = '/FileStore/tables/v1a51v731472210317176/sweng_dataset.json'\n",
    "filename_database = '/FileStore/tables/v1a51v731472210317176/database_dataset.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleans input data frame of all unnecessary characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(input_df):\n",
    "  onlywords_df = input_df.select(funs.lower(\n",
    "      funs.regexp_replace(\n",
    "        input_df.text,'(\\^|\\?|\\!|\\$|\\(|\\)|\\\\b\\\\w{1}\\\\b\\\\s?|\\\"|\\'|RT|:|\\.|@[^\\s]+|http[^\\s]+|#|[0-9]+)','')).alias('text'),'label','id')\n",
    "  return onlywords_df.select(funs.trim(funs.regexp_replace(onlywords_df.text,'(\\+|_|-|\\,|\\s+)',' ')).alias('text'),'label','id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For input data frame count number of positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_posneg(input_df):\n",
    "  return {\"all\":input_df.count(),\"positive\":input_df.filter(input_df.label==1).count(),\"negative\":input_df.filter(input_df.label==0).count()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extends input data frame with one additional column 'user_description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_missing_column(input_df):\n",
    "  return input_df.rdd.map(lambda row:[row['id'],row['hashtags'],row['text'],row['label'],'']).toDF(['id','hashtags','text','label','user_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine string with list of strings\n",
    "'Some text', ['one', 'two', 'three'] ----> Some text one two three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_hash_text(text,l):\n",
    "  return text+' '+' '.join(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_datastax_df = sqlContext.read.json(filename_datastax)\n",
    "raw_porn_df = sqlContext.read.json(filename_porn)\n",
    "raw_cassandra_df = sqlContext.read.json(filename_cass)\n",
    "raw_spark_df = sqlContext.read.json(filename_spark)\n",
    "raw_bigdata_df = sqlContext.read.json(filename_bigdata)\n",
    "raw_softeng_df = sqlContext.read.json(filename_softeng)\n",
    "raw_database_df = sqlContext.read.json(filename_database)\n",
    "raw_cassandra_update_df = sqlContext.read.json(filename_update)\n",
    "\n",
    "print 'Datastax tweets:\\t',count_posneg(raw_datastax_df)\n",
    "print 'Porn tweets:\\t',count_posneg(raw_porn_df)\n",
    "print 'Cassandra tweets:\\t',count_posneg(raw_cassandra_df)\n",
    "print 'Spark tweets:\\t',count_posneg(raw_spark_df)\n",
    "print 'Bigdata tweets:\\t',count_posneg(raw_bigdata_df)\n",
    "print 'Software eng:\\t',count_posneg(raw_softeng_df)\n",
    "print 'Database:\\t',count_posneg(raw_database_df)\n",
    "print 'Cassandra_update:\\t',count_posneg(raw_cassandra_update_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some of tweets from dataset, for better positive/negative ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_datastax_df = (raw_datastax_df.filter(raw_datastax_df.label==1)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_spark_pos_df = (raw_spark_df.filter(raw_spark_df.label==1)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_spark_neg_df = (raw_spark_df.filter(raw_spark_df.label==0)).randomSplit([0.5,0.5],23)[0]\n",
    "sampled_bigdata_df = raw_bigdata_df.randomSplit([0.3,0.7],23)[0]\n",
    "sampled_softeng_df = raw_softeng_df.randomSplit([0.3,0.7],23)[0]\n",
    "sampled_database_df = raw_database_df.randomSplit([0.1,0.7],23)[0]\n",
    "\n",
    "print 'Sampled Datastax',sampled_datastax_df.count()\n",
    "print 'Sampled Spark Positive',sampled_spark_pos_df.count()\n",
    "print 'Sampled Spark Negative',sampled_spark_neg_df.count()\n",
    "print 'Sampled Bigdata',sampled_bigdata_df.count()\n",
    "print 'Sampled Software Eng',sampled_softeng_df.count()\n",
    "print 'Sampled database',sampled_database_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing columns\n",
    "(only for datasets that dont have user_description, to avoid error in merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_datastax_df = add_missing_column(sampled_datastax_df)\n",
    "sampled_database_df = add_missing_column(sampled_database_df)\n",
    "sampled_bigdata_df = add_missing_column(sampled_bigdata_df)\n",
    "sampled_softeng_df = add_missing_column(sampled_softeng_df)\n",
    "sampled_spark_neg_df = add_missing_column(sampled_spark_neg_df)\n",
    "sampled_spark_pos_df = add_missing_column(sampled_spark_pos_df)\n",
    "sampled_porn_df = add_missing_column(raw_porn_df).select('id','hashtags','text','label','user_description')\n",
    "sampled_cass_df = add_missing_column(raw_cassandra_df).select('id','hashtags','text','label','user_description')\n",
    "sampled_cass_u_df = raw_cassandra_update_df.select('id','hashtags','text','label','user_description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets into big one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_df = sampled_bigdata_df.unionAll(sampled_database_df).unionAll(sampled_cass_df).unionAll(sampled_datastax_df).unionAll(sampled_porn_df).unionAll(sampled_softeng_df).unionAll(sampled_spark_neg_df).unionAll(sampled_spark_pos_df).unionAll(sampled_cass_u_df)\n",
    "dataset_df.show()\n",
    "print count_posneg(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine text, hashtags and user_description columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_combined_df = (dataset_df.rdd\n",
    "              .map(lambda row:[row['id'],float(row['label']),combine_hash_text(row['text'],row['hashtags'])+' '+row['user_description']])).toDF(['id','label','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataframe of thrash characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dataset_df = clean_dataframe(dataset_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Split with cv'''\n",
    "'''\n",
    "split_ratio = [0.6,0.2,0.2]\n",
    "split_seed = 212\n",
    "train_df,cv_df,test_df = cleaned_dataset_df.randomSplit(split_ratio,split_seed)\n",
    "train_df.cache()\n",
    "cv_df.cache()\n",
    "test_df.cache()\n",
    "print 'Train set:',count_posneg(train_df)\n",
    "print 'Cv set:',count_posneg(cv_df)\n",
    "print 'Test set:',count_posneg(test_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df = cleaned_dataset_df.randomSplit([0.8,0.2],213)\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform test for given parameters and return results of classification on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_models(train_df,cv_df,feature_nums,regularizations,thresholds):\n",
    "  output = []\n",
    "  tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "  hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "  for ft in feature_nums:\n",
    "    train_tokens_df = tokenizer.transform(train_df)\n",
    "    cv_tokens_df = tokenizer.transform(cv_df)\n",
    "    hashingTF.setNumFeatures(ft)\n",
    "    hashed_train_df = hashingTF.transform(train_tokens_df)\n",
    "    hashed_cv_df = hashingTF.transform(cv_tokens_df)\n",
    "    train_labels = hashed_train_df.rdd.map(lambda row: LabeledPoint(row['label'],row['features']))\n",
    "    for threshold in thresholds:\n",
    "      for reg in regularizations:\n",
    "        model = LogisticRegressionWithLBFGS()\n",
    "        trained = model.train(train_labels,regParam=reg)\n",
    "        trained.setThreshold(threshold)\n",
    "        predicted = hashed_cv_df.rdd.map(lambda l:(float(trained.predict(l['features'])),float(l['label']),l['text'],l['id']))\n",
    "        predictionAndLabels_rdd = predicted.map(lambda lp: (lp[0], lp[1]))\n",
    "\n",
    "        metrics = BinaryClassificationMetrics(predictionAndLabels_rdd)\n",
    "        entry = {'features':ft, 'regularization':reg,'threshold':threshold,'under ROC':metrics.areaUnderROC,'under PR':metrics.areaUnderPR}\n",
    "        output.append(entry)\n",
    "    \n",
    "   \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#results = train_models(train_df,test_df,[800,1600],[0.01,0.04,0.09,0.1,0.13,0.15],[0.7,0.75])\n",
    "#for res in results:\n",
    "#  print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and HashTF words in text of dataframe, and then train LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF.setNumFeatures(1600)\n",
    "tokenized = tokenizer.transform(train_df)\n",
    "hashed = hashingTF.transform(tokenized)\n",
    "train_labels = hashed.rdd.map(lambda row: LabeledPoint(row['label'],row['features']))\n",
    "lr = LogisticRegressionWithLBFGS()\n",
    "model = lr.train(train_labels,regParam=0.04)\n",
    "model.setThreshold(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare, predict and print test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.clearThreshold()\n",
    "hashed_test = hashingTF.transform(tokenizer.transform(test_df))\n",
    "predicted = hashed_test.rdd.map(lambda l:(float(model.predict(l['features'])),float(l['label']),l['text'],l['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pred in predicted.collect():\n",
    "  print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save(sc,'/FileStore/models/lr_29_8.lrm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "log_reg_2",
  "notebookId": 2452108591016848
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
