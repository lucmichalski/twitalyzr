{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming of Tweets from Twitter Api and performe classification\n",
    "This example uses text of tweets, user description and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "import twitter\n",
    "import dateutil.parser\n",
    "import json\n",
    "from pyspark.sql import functions as funs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(input_df):\n",
    "  onlywords_df = input_df.select(funs.lower(\n",
    "      funs.regexp_replace(\n",
    "        input_df.text,'(\\^|\\?|\\!|\\$|\\(|\\)|\\\\b\\\\w{1}\\\\b\\\\s?|\\\"|\\'|RT|:|\\.|@[^\\s]+|http[^\\s]+|#|[0-9]+)','')).alias('text'),'id','hashtags')\n",
    "  return onlywords_df.select(funs.trim(funs.regexp_replace(onlywords_df.text,'(\\+|_|-|\\,|\\s+)',' ')).alias('text'),'id','hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_hash_text(text,l):\n",
    "  return text+' '+' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(dict):\n",
    "  def __init__(self, tweet_in):\n",
    "    super(Tweet, self).__init__(self)\n",
    "    if tweet_in and tweet_in['lang']=='en':\n",
    "      self['timestamp'] = dateutil.parser.parse(tweet_in[u'created_at']).replace(tzinfo=None).isoformat()\n",
    "      self['text'] = tweet_in['text']\n",
    "      self['hashtags'] = []\n",
    "      for hash in tweet_in['entities']['hashtags']:\n",
    "        self['hashtags'].append(hash['text'])\n",
    "      self['urls'] = [x for x in tweet_in['entities']['urls']]\n",
    "      self['id'] = tweet_in['id']\n",
    "      self['screen_name'] = tweet_in['user']['screen_name'].encode('utf-8')\n",
    "      self['user_description'] = tweet_in['user']['description']\n",
    "      self['user_id'] = tweet_in['user']['id']\n",
    "    else:\n",
    "      self['timestamp'] = ''\n",
    "      self['text'] = 'No TEXT'\n",
    "      self['hashtags'] = []\n",
    "      self['urls'] = []\n",
    "      self['id'] = tweet_in['id']\n",
    "      self['screen_name'] = tweet_in['user']['screen_name']\n",
    "      self['user_description'] = ''\n",
    "      self['user_id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_twitter():\n",
    "  twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(\n",
    "  token = \"...\",\n",
    "  token_secret = \"...\",\n",
    "  consumer_key = \"...\",\n",
    "  consumer_secret = \"...\"))\n",
    "  return twitter_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_tweet(twitter_stream):\n",
    "  #stream = twitter_stream.statuses.sample(block=True)\n",
    "  stream = twitter_stream.statuses.filter(track=\"#universalchampionship\",lang='en')\n",
    "  tweet_in = None\n",
    "  while not tweet_in or 'delete' in tweet_in:\n",
    "    tweet_in = stream.next()\n",
    "  tweet_parsed = Tweet(tweet_in)\n",
    "  print tweet_parsed\n",
    "  return json.dumps(tweet_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(time,rdd):\n",
    "  input_df = (rdd.map(lambda json_str:(json.loads(json_str))).map(lambda r:Row(r['text'],r['id'],r['hashtags'],r['user_description']))).toDF(['text','id','hashtags','user_description'])\n",
    "  \n",
    "  combined_df = (input_df.rdd\n",
    "              .map(lambda row:[row['id'],combine_hash_text(row['text'],row['hashtags'])+' '+row['user_description'],row['hashtags']])).toDF(['id','text','hashtags'])\n",
    "  \n",
    "  cleaned_df = clean_dataframe(combined_df)\n",
    "  \n",
    "  cleaned_df.show()\n",
    "  \n",
    "  tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "  hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "  \n",
    "  hashingTF.setNumFeatures(1600)\n",
    "  tokenized = tokenizer.transform(cleaned_df)\n",
    "  hashed = hashingTF.transform(tokenized)\n",
    "  \n",
    "  hashed.show()\n",
    "  return hashed.rdd\n",
    "  \n",
    "  #features = feature_extraction_with_hash(cleaned_df,400,1900)\n",
    "  #features.show()\n",
    "  #return features.rdd\n",
    "  \n",
    "  \n",
    "def process_rdd_queue(twitter_stream,lr_model):\n",
    "  # Create the queue through which RDDs can be pushed to\n",
    "  # a QueueInputDStream\n",
    "  rddQueue = []\n",
    "  \n",
    "  '''\n",
    "  Wait 5 tweet then finish\n",
    "  '''\n",
    "  for i in range(1):\n",
    "    rddQueue +=[ssc.sparkContext.parallelize([get_next_tweet(twitter_stream)], 5)]\n",
    "  \n",
    "  #(ssc.queueStream(rddQueue).transform(getFeatures).transform(lambda _, rdd:rdd).map(lambda row:(lr_model.predict(row['features']),row['text']))).pprint()\n",
    "  #lines.pprint()\n",
    "  (ssc.queueStream(rddQueue).transform(getFeatures).transform(lambda _, rdd:rdd).map(lambda row:(lr_model.predict(row['features']),row['text']))).pprint()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)\n",
    "# Instantiate the twitter_stream\n",
    "twitter_stream = connect_twitter()\n",
    "# Get RDD queue of the streams json or parsed\n",
    "filename_model = '/FileStore/models/lr_29_8.lrm'\n",
    "lr_model = LogisticRegressionModel.load(sc,filename_model)\n",
    "lr_model.clearThreshold()\n",
    "process_rdd_queue(twitter_stream,lr_model)\n",
    "ssc.start()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "twitter_stream_tweetext",
  "notebookId": 2403114416148339
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
