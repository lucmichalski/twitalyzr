{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter stream classification on #cassandra using Spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer\n",
    "from pyspark.sql import Row\n",
    "import twitter\n",
    "import dateutil.parser\n",
    "import json\n",
    "from pyspark.sql import functions as funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(input_df):\n",
    "    onlywords_df = input_df.select(funs.lower(\n",
    "        funs.regexp_replace(\n",
    "        input_df.text,'(\\^|\\?|\\!|\\$|\\(|\\)|\\\\b\\\\w{1}\\\\b\\\\s?|\\\"|\\'|RT|:|\\.|@[^\\s]+|http[^\\s]+|#|[0-9]+)','')).alias('text'),'id')\n",
    "    return onlywords_df.select(funs.trim(funs.regexp_replace(onlywords_df.text,'(\\+|_|-|\\,|\\s+)',' ')).alias('text'),'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenization(input_df):\n",
    "    tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    return tokenizer.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(dict):\n",
    "    def __init__(self, tweet_in):\n",
    "        super(Tweet, self).__init__(self)\n",
    "            if tweet_in and tweet_in['lang']=='en':\n",
    "                self['timestamp'] = dateutil.parser.parse(tweet_in[u'created_at']).replace(tzinfo=None).isoformat()\n",
    "                self['text'] = tweet_in['text']\n",
    "                self['hashtags'] = []\n",
    "                for hash in tweet_in['entities']['hashtags']:\n",
    "                    self['hashtags'].append(hash['text'])\n",
    "                self['urls'] = [x for x in tweet_in['entities']['urls']]\n",
    "                self['id'] = tweet_in['id']\n",
    "                self['screen_name'] = tweet_in['user']['screen_name'].encode('utf-8')\n",
    "                self['user_description'] = tweet_in['user']['description']\n",
    "                self['user_id'] = tweet_in['user']['id']\n",
    "            else:\n",
    "                self['timestamp'] = ''\n",
    "                self['text'] = 'No TEXT'\n",
    "                self['hashtags'] = []\n",
    "                self['urls'] = []\n",
    "                self['id'] = tweet_in['id']\n",
    "                self['screen_name'] = tweet_in['user']['screen_name']\n",
    "                self['user_description'] = ''\n",
    "                self['user_id'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill this for twitter auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_twitter():\n",
    "    twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(\n",
    "    token = \"...\",\n",
    "    token_secret = \"...\",\n",
    "    consumer_key = \"...\",\n",
    "    consumer_secret = \"...\"))\n",
    "    return twitter_stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(sv1,sv2):\n",
    "    size = sv1.size + sv2.size\n",
    "    max_ind = sv1.size\n",
    "    indices1 = sv1.indices\n",
    "    indices2 = [max_ind+x for x in sv2.indices]\n",
    "    joined_indices = np.concatenate([indices1,indices2])\n",
    "    values1 = sv1.values\n",
    "    values2 = sv2.values\n",
    "    joined_values = np.concatenate([values1,values2])\n",
    "    return SparseVector(size,joined_indices,joined_values)\n",
    "\n",
    "s1 = SparseVector(2,[0],[1])\n",
    "s2 = SparseVector(3,[0,2],[1,1])\n",
    "print combine(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_word(hashtags):\n",
    "    hashtags = [x.lower() for x in hashtags]\n",
    "    return ' '.join(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_with_hash(input_df,number_hash,number_words):\n",
    "    lower_hashtags = (input_df.rdd.map(lambda row:[lower_word(row['hashtags']),row['text'],row['id']])).toDF(['hashtags','text','id'])\n",
    "  \n",
    "    #tokenization\n",
    "    tok = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    tok_hash = RegexTokenizer(inputCol='hashtags',outputCol='tags')\n",
    "  \n",
    "    tokenized_test_words = tok.transform(lower_hashtags)\n",
    "    tokenized_all = tok_hash.transform(tokenized_test_words)\n",
    "  \n",
    "    #hashingTF\n",
    "    tags_hash = HashingTF(inputCol=\"tags\", outputCol=\"features_tags\", numFeatures=number_hash)\n",
    "    words_hash = HashingTF(inputCol=\"words\", outputCol=\"features_words\", numFeatures=number_words)\n",
    "    tf_feat = tags_hash.transform(tokenized_all)\n",
    "    tf_all = words_hash.transform(tf_feat)\n",
    "  \n",
    "    #combine SparseVectors\n",
    "    tf_joined = (tf_all.rdd.map(lambda row:[combine(row['features_tags'],row['features_words']),row['text'],row['hashtags'],row['id']])).toDF(['features','text','hashtags','id'])\n",
    "  \n",
    "    return tf_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_tweet(twitter_stream):\n",
    "    #stream = twitter_stream.statuses.sample(block=True)\n",
    "    stream = twitter_stream.statuses.filter(track=\"#cassandra\")\n",
    "    tweet_in = None\n",
    "    while not tweet_in or 'delete' in tweet_in:\n",
    "        tweet_in = stream.next()\n",
    "    tweet_parsed = Tweet(tweet_in)\n",
    "    print tweet_parsed\n",
    "    return json.dumps(tweet_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as funs\n",
    "'''\n",
    "def process(time,rdd):\n",
    "  input_df = (rdd.map(lambda json_str:(json.loads(json_str))).map(lambda r:Row(r['text'],r['id']))).toDF(['text','id'])\n",
    "  cleaned_df = clean_dataframe(input_df)\n",
    "  #cleaned_df.show()\n",
    "  tokenized = tokenization(cleaned_df)\n",
    "  #tokenized.show()\n",
    "  hashing_tf_model = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=1000)\n",
    "  tfidf = hashing_tf_model.transform(tokenized)\n",
    "\n",
    "  #idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "  #idf_model = idf.fit(hashed_tf_df)\n",
    "  #tfidf = idf_model.transform(hashed_tf_df)\n",
    "  #tfidf.show()\n",
    "\n",
    "  lr_model = LogisticRegressionModel.load(sc,'/FileStore/tables/92zrseiu1471850178155/lr_model2.mdl')\n",
    "  lr_model.clearThreshold()\n",
    "  prediction = tfidf.rdd.map(lambda lp: (float(lr_model.predict(lp['features'])),lp['text']))\n",
    "  print 'Prediction:',prediction.take(1)\n",
    "  return prediction\n",
    "'''\n",
    "\n",
    "def getFeatures(time,rdd):\n",
    "    input_df = (rdd.map(lambda json_str:(json.loads(json_str))).map(lambda r:Row(r['text'],r['id']))).toDF(['text','id'])\n",
    "    cleaned_df = clean_dataframe(input_df)\n",
    "    tokenized = tokenization(cleaned_df)\n",
    "    hashing_tf_model = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=1000)\n",
    "    tfidf = hashing_tf_model.transform(tokenized)\n",
    "    tfidf.show()\n",
    "  \n",
    "    return tfidf.rdd\n",
    "\n",
    "def getFeaturesWithTags(time,rdd):\n",
    "    input_df = (rdd.map(lambda json_str:(json.loads(json_str))).map(lambda r:Row(r['text'],r['id'],r['hashtags']))).toDF(['text','id','hashtags'])\n",
    "  \n",
    "    cleaned_df = clean_dataframe(input_df)\n",
    "    features = feature_extraction_with_hash(cleaned_df,400,1900)\n",
    "    features.show()\n",
    "    return features.rdd\n",
    "  \n",
    "def process_rdd_queue(twitter_stream,lr_model):\n",
    "    # Create the queue through which RDDs can be pushed to\n",
    "    # a QueueInputDStream\n",
    "    rddQueue = []\n",
    "  \n",
    "    '''\n",
    "    Wait 5 tweet then finish\n",
    "    '''\n",
    "    for i in range(1):\n",
    "        rddQueue +=[ssc.sparkContext.parallelize([get_next_tweet(twitter_stream)], 5)]\n",
    "  \n",
    "    #(ssc.queueStream(rddQueue).transform(getFeatures).transform(lambda _, rdd:rdd).map(lambda row:(lr_model.predict(row['features']),row['text']))).pprint()\n",
    "    #lines.pprint()\n",
    "    (ssc.queueStream(rddQueue).transform(getFeaturesWithTags).transform(lambda _, rdd:rdd).map(lambda row:(lr_model.predict(row['features']),row['text']))).pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change file path for loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "# Instantiate the twitter_stream\n",
    "twitter_stream = connect_twitter()\n",
    "# Get RDD queue of the streams json or parsed\n",
    "filename_model = '/FileStore/tables/oaohfspx1472031253448/trained_model.lrm'\n",
    "lr_model = LogisticRegressionModel.load(sc,filename_model)\n",
    "lr_model.clearThreshold()\n",
    "process_rdd_queue(twitter_stream,lr_model)\n",
    "ssc.start()\n",
    "time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "twitter_streaming",
  "notebookId": 4369489841068477
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
